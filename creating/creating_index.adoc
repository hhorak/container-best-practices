// vim: set syntax=asciidoc:
[[create]]
== Creating Images
:data-uri:
:toc:
:toclevels 4:
:homepage https://github.com/projectatomic/container-best-practices:

The base unit of creating an image is the Dockerfile itself.  This section
focuses on the instructions that make up a Dockerfile.

This chapter will not cover every Dockerfile instruction available but instead
will focus on specific ones that we want to re-enforce to those who develop
Dockerfiles.  Docker has published a
link:https://docs.docker.com/engine/reference/builder/[reference guide] already
covering each of the Dockerfile instructions. In addition, upstream docker has a nice description of https://docs.docker.com/engine/articles/dockerfile_best-practices/[best practices] for Dockerfiles. It
describes the various instructions that can be used to compose a Dockerfile and their best usage. Familiarize yourself with these
recommendations.

=== Creating Base Images

==== Choosing Base Image

Images that have no parent are called base images. Docker image usually have their own root filesystem with an operating system installed. So when you want to create a new image, it either has to be based on an image that actually provides an operating system or you will need to create this layer in your image. The only difference to this are super minimal images that instead of an operating system provide only a single binary as described later in the text.
There is a wide variety of base images already available on Docker Hub, so the simplest solution is to use one from there. Here are a few things that should help you determine which base image will fit your needs:

* Linux distribution - Your personal preference and perhaps experience is a reason why to choose a certain distribution rather than another one. However, you should definitely consider whether your containerized application requires specific libraries or tools from a specific system.

* Image size - Base images usually contain a minimal operating system with a set of tools needed for basic operations. To preserve your environment small and efficient, size should also be taken into account when picking the right base image. The size varies; you can take advantage of super small base images, such as 2MB busybox, or use a standard minimal operating system, such as Fedora or CentOS that are up to 200MB in size.

* Updates - Not all community images are necessarily rebuilt on a regular basis or when security vulnerabilities are addressed. You should therefore consider using base images from "official repositories" on Docker Hub, and confirm their update policy in advance.

//==== RHEL base images

// ==== BusyBox, the Minimalistic Operating System

==== Creating Base Image

Once you've considered all options and decided to create your own base image, the process will mostly depend on the distribution you chose. Note that the major distributions have their source files available on GitHub so you still might want to consider creating an issue or opening a pull request to suggest a change in the feature set or any adjustment.
https://docs.docker.com/engine/userguide/eng-image/baseimages/[Docker documentation] suggests two approaches to creating a base image, using tar and building an image "FROM scratch".

===== Using tar

Using the tar tool is a simple way how to build a base image. As a prerequisite, you will need to set up a directory structure for chroot with all items that you wish to be part of the base image. There are various tools that might help you with this, for example _debootstrap_ for Debian systems or _supermin_ for RPM-based systems.

Once you have your chroot directory ready, it is as simple as running:

```
# tar -C <chroot_dir> -c . | docker import - <new_image_name>
```

Note that docker provides a set of scripts for base image creation that take advantage of tar: https://github.com/docker/docker/tree/master/contrib[https://github.com/docker/docker/tree/master/contrib]. Well known distributions then use their own build systems that usually also utilizes tar. For example Fedora's https://fedoraproject.org/wiki/Koji/BuildingImages?rd=Koji/KojiLiveCDHowTo#Building_Disk_Images[koji].

===== FROM scratch

"scratch" is a special repository in the Docker Hub registry, created using an empty tarball. It is not meant to be pulled or run, and at any such an attempt you will most likely encounter this message: _'scratch' is a reserved name_.
Using scratch is ideal for creating extremely minimal images, for example for containerizing single binaries. An example is available from https://docs.docker.com/engine/userguide/eng-image/baseimages/[Docker documentation].
scratch is also very handy for creating standard distribution base images. But as with tar, you'll first need to prepare a directory structure for chroot. After that, just add the directory in your Dockerfile as follows:

```
FROM scratch
ADD <chroot_dir> /
CMD ["/bin/bash"]
```

=== Creating Layered Images

==== Creating Component or Application Images

[[creating_concise]]
=== Create small and concise images

It is preferable to create small and concise images whenever possible.  This can
be highly dependent on the application you are containerizing, but there are
techniques to help you accomplish this.  The following sections cover these
techniques.

==== Clear packaging caches and temporary package downloads

Package managers can typically generate lots of metadata and also store downloaded content into a cache of
sorts. To keep images and layers as small as possible, you should consider clearing out these caches of downloaded
content.  Note how the following example ends with a _yum -y clean all_ which removes deletable yum content.

.A singular RUN instruction performing multiple commands
```
RUN yum install -y epel-release && \
    rpmkeys --import file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 && \
    yum install -y --setopt=tsflags=nodocs bind-utils gettext iproute\
    v8314 mongodb24-mongodb mongodb24 && \
    yum -y clean all
```

There are several package managers beyond yum that should be of note: dnf, rvm, gems, cpan, pip. Most of these
managers have some form of a clean up command that will handle excess cache created while performing their package management duties.

Below are examples pictured for dnf and rvm:

.dnf cleanup example
```
RUN rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm && \
    dnf -y install nodejs tar sudo git-all memcached postgresql-devel postgresql-server \
    libxml2-devel libxslt-devel patch gcc-c++ openssl-devel gnupg curl which && \
    dnf clean all && \
```

.Ruby(rvm) cleanup example
```
RUN /usr/bin/curl -sSL https://rvm.io/mpapis.asc | gpg2 --import - && \
    /usr/bin/curl -sSL https://get.rvm.io | rvm_tar_command=tar bash -s stable && \
    source /etc/profile.d/rvm.sh && \
    echo "gem: --no-ri --no-rdoc --no-document" > ~/.gemrc && \
    /bin/bash -l -c "rvm requirements && rvm install ruby 2.2.4 && rvm use 2.2.4 --default && \
    gem install bundler rake && \
    gem install nokogiri --use-system-libraries && \
    rvm cleanup all && yum clean all && rvm disk-usage all"
```

In the above example, notice the yum clean all called after rvm, this is because some package managers like rvm rely on others (like yum
in this case) to help perform their duties. Make sure to examine your container's layers sizes to help determine where you can
eliminate excess size and keep it's footprint size to a minimum.

Here is a listing of some package managers and the applicable cleanup commands:

.Package Managers
[cols="2*", options"header"]
|===
|Package Manager
|Cleanup Command

|yum
|yum clean all

|dnf
|dnf clean all

|rvm
|rvm cleanup all

|gem
|gem cleanup

|cpan
|rm -rf ~/.cpan/{build,sources}/*

|pip
|rm -rf ~/.cache/pip/*

|apt-get
|apt-get clean

|===

===== Clearing package cache and squashing

If you squash your images after manual building or as part of an automated build process, it is not necessary to clean cache in every single relevant instruction/layer as the intermediate layers affect the previous ones in this case.

Simple example Dockerfiles below would both produce the same image if they were squashed:

.Cache cleanup in a separate instruction
```
FROM fedora
RUN dnf install -y mariadb
RUN dnf install -y wordpress
RUN dnf clean all
```

.Cache cleanup chained with the install command
```
FROM fedora
RUN dnf install -y mariadb wordpress && dnf clean all

```
However, without squashing, the first image would contain additional files and would be bigger than the second one.

.Size comparison
```
# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED              VIRTUAL SIZE
example             separate            54870d73715f        21 seconds ago       537.7 MB
example             chained             6a6156547888        About a minute ago   377.9 MB

```

Therefore, it is a good practice to write Dockerfiles in a way so that others can use it as a valid reference and are always able to reproduce the build. To ensure this, you should **clean cache in every layer** where applicable. In general, you should always aim to create images that are small and concise regardless of whether the final image is squashed or not.

Read more about suqashing and its repercussions in the link:#squashing[Squashing layers] section.

==== Remove unnecessary packages

In some cases, your image can end up with several packages that are not necessary to support the runtime of your
application.  A good example is when you actually build your application from source during the build of the image
itself.  Typically, when you build an application, you will pull in development (-devel) packages as well as
toolchain based packages like make and gcc.  Once your application is built, you may no longer need these packages
for runtime depending on how your application links to libraries.

Depending on your application and which packages you added to your image, you might need to iteratively attempt to
remove packages checking to make sure your application still works.  One suggestion would be to remove big parts of the
toolchain.  And then use your package manager's command to clean up unused packages.  In the case of _yum_, you can
remove unneeded packages like so:

.Removing unnecessary packages with yum
```
# yum autoremove
```

You should run this command in an interactive shell (docker run -it --rm <image> /bin/bash) initially so you can
get a feel for which packages will be removed.  One upside to doing so is that you can then test run your application
from the interactive shell to make sure it still works.

==== Installing Documentation
It is generally considered good practice to keep your images as small as possible.  Above we have discussed that
package manager caches should be cleared to reduce image sizes.  You can also reduce image size by limiting the
documentation being installed.  If you package manager supports such a thing and then you have no expectations
for users to use a shell to interact with your image, this might significantly reduce the size of your image.

Yum has an optional flag to not install documentation.  The following example shows how to set the flag.
----
RUN yum install -y mysql --setopt=tsflags=nodocs
----

Note that the **nodocs** flag is used in some base images, for example CentOS and Fedora, and this setting gets
inherited by the child layers. This can cause problems in case you want to include documentation deliberately in your
 layered image.

In this case, if you wish to have the documentation installed for **packages from your single layer only**, you have to
empty the **tsflags** option as follows:

----
RUN yum -y install docker --setopt=tsflags=''
----

If you wish to have the documentation installed for **packages from your single layer and the parent layers**, you need
 to reinstall the packages with the empty **tsflags** option as follow:

----
RUN yum -y reinstall "*" --setopt-tsflags='' && yum install docker --setopt-tsflags=''
----

In case you need to have documentation included for **every package from every single parent or child layer**,
the */etc/yum.conf* file needs to be edited as follows:

----
RUN [ -e /etc/yum.conf ] && sed -i '/tsflags=nodocs/d' /etc/yum.conf || true
RUN yum -y reinstall "*"
RUN yum -y install <package>
----

[[squashing]]
==== Squashing layers

Each instruction you create in your Dockerfile results in a new image layer being created. Each layer brings additional
data that are not always part of the resulting image. For example, if you add a file in one layer, but remove it in
another layer later, the final image's size will include the added file size in a form of a special "whiteout" file
although you removed it. In addition, every layer contains separate metadata that add up to the overall image size as
well. So what are the benefits of squashing?

* **Performance** - Since all layers are copy-on-write file systems, it will take longer to build the final container
from many layers. Squashing helps reduce the build time.

* **Image size** - Similarly, since an image is actually a collection of other images, the final image size is the sum of the sizes of component images. With squashing, you can prevent these unwanted size additions.

* **Organization** - Squashing also helps you control the structure of an image, reduce the number of layers and organize images logically.

However, Docker does not yet support squashing natively, so you will have to work around it by using alternative
approaches, some of which are listed below.

===== docker save

You can use _docker save_ to squash all the layers of your image into a single layer.  The _save_ command
was intended for this use, so this happens to be a side effect of the process. This approach,
however, is not very practical for sharing as the user will be able to only download
the whole content and cannot take advantage the caching. Note that the base image layer will be included
as well and might be several hundreds of megabytes in size.

[[squash_tools]]
===== Custom Tools

You will surely find a lot of utilities on the internet that facilitate layer squashing.
We recommend taking advantage of Marek Goldmann's https://github.com/goldmann/docker-squash[docker-squash], which
automates layer squashing and which is maintained and has been tested by the community.


===== Repercussions of squashing

* When you squash an image, you will lose the history together with the metadata accompanying the layers.
* Without the metadata, users building an image from a layered image that has been squashed are losing the idea that it happened.
* Similarly, if you decide to include the parent layer from which your image is built into the resulting squashed image, you ultimately prevent others from seeing that this happened.

Look at the mongodb example:

```
# docker images openshift/mongodb-24-centos7
REPOSITORY                               TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
docker.io/openshift/mongodb-24-centos7   latest              d7c0c18b0ae4        16 hours ago        593.3 MB
```

Without squashing, you can see complete history and how each of the layers occupies space.

```
# docker history docker.io/openshift/mongodb-24-centos7:latest
IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT
d7c0c18b0ae4        About an hour ago   /bin/sh -c #(nop) CMD ["run-mongod"]            0 B
63e2ba112add        About an hour ago   /bin/sh -c #(nop) ENTRYPOINT &{["container-en   0 B
ca996db9c281        About an hour ago   /bin/sh -c #(nop) USER [184]                    0 B
8593b9473058        About an hour ago   /bin/sh -c #(nop) VOLUME [/var/lib/mongodb/da   0 B
5eca88b7872d        About an hour ago   /bin/sh -c touch /etc/mongod.conf && chown mo   0 B
9439db8f40ad        About an hour ago   /bin/sh -c #(nop) ADD dir:f38635e83f0e6943cd3   17.29 kB
12c60945cbac        About an hour ago   /bin/sh -c #(nop) ENV BASH_ENV=/usr/share/con   0 B
e6073f9a949f        About an hour ago   /bin/sh -c #(nop) ENV CONTAINER_SCRIPTS_PATH=   0 B
619bf2ae5ed8        About an hour ago   /bin/sh -c yum install -y centos-release-scl    342.6 MB
ab5deeccfe21        About an hour ago   /bin/sh -c #(nop) EXPOSE 27017/tcp              0 B
584ded9dcbca        About an hour ago   /bin/sh -c #(nop) LABEL io.k8s.description=Mo   0 B
17e3bcd28e07        About an hour ago   /bin/sh -c #(nop) ENV MONGODB_VERSION=2.6 HOM   0 B
807a1e9c5a7b        16 hours ago        /bin/sh -c #(nop) MAINTAINER SoftwareCollecti   0 B
28e524afdd05        10 days ago         /bin/sh -c #(nop) CMD ["/bin/bash"]             0 B
044c0f15c4d9        10 days ago         /bin/sh -c #(nop) LABEL name=CentOS Base Imag   0 B
2ebc6e0c744d        10 days ago         /bin/sh -c #(nop) ADD file:6dd89087d4d418ca0c   196.7 MB
fa5be2806d4c        7 months ago        /bin/sh -c #(nop) MAINTAINER The CentOS Proje   0 B

```
See how the history and size changes after squashing all layers in a single one (using the script link:#squash_tools[above]):

```
# docker history docker.io/openshift/mongodb-24-centos7:squashed
IMAGE               CREATED             CREATED BY          SIZE                COMMENT
90036ed9bd1d        58 minutes ago                          522.1 MB

```

* One of the biggest benefits of using layers is the posibility to reuse them. Images are usually squashed into a single big layer, which does not allow for pushing partial updates in individual layers; instead, the whole image needs to be pushed into the registry upon a change. The same applies to pulling the image from the registry.
* Some users might rely on suqashing when it comes to sensitive data. Be cautios because squashing is not meant to "hide" content. Even though squashing removes intermediate layers from the final image, information about secrets used in those layers will stay in the build cache.

==== Chaining Commands

In general, having fewer layers improves readability. Commands that are chained together become a part of the
same layer. To reduce the number of layers, chain commands together. Find a balance, though, between a large
number of layers (and a great many commands), and a small number of layers (and obscurity caused by brevity).

A new layer is created for every new instruction defined. This does not necessarily mean that one instruction
should be associated with only one command or definition.

Ensure transparency and provide a good overview of the content of each layer by grouping related operations
together so that they together constitute a single layer. Consider this snippet:

.Chained Dockerfile instruction
```
RUN yum install -y --setopt=tsflags=nodocs \
    httpd vim && \
    systemctl enable httpd &&
    yum clean all
```

Each command that is related to the installation and configuration of `httpd` is grouped together
as a part of the same layer. This meaningful grouping of operations keeps the number of layers low
while keeping the easy legibility of the layers high.

===== Using semi-colons (;) vs double ampersands (&&)

In the RUN instruction of Dockerfiles, it is common to string together multiple commands for efficiency.  Stringing
commands together in the RUN instructions are typically done with ampersands or semi-colons. However, you should
consider the implications of each and their usage.  The following examples illustrate the difference.

.Using semi-colons as instruction conjunctions
```
RUN do_1; do_2
```

This sort of conjunction will be evaluated into do_1 and then do_2.  However, using the double
ampersands results in a different evaluation.

.Using double ampersands as conjunctions
```
RUN do_1 && do_2
```

The ampersands change the resulting evaluation into do_1 and then do_2 _only if do_1 was successful_.

The use of the double ampersands as conjunctions is probably a better practice in Dockerfiles because
it ensures that your instructions are completed or the build will fail.  If the build were to continue
and you had not closely monitored the build (or its results), then the image may not be exactly
as you desired.  This is particularly true with automated build systems where you will want any
failure to result in the failure of the build itself.

There are certainly use cases where semi-colons might be preferred and possibly should be used.
Nevertheless, the possible result of an incomplete image should be carefully considered.


==== Locales

=== Labels

Labels in Dockerfiles serve as a useful way to organize and document metadata used to describe an image.  Some labels are only descriptive
by nature, like  _Name_ whereas others, like _RUN_ can be used to describe action-oriented metadata.  Labels are often leveraged by applications, like
https://github.com/projectatomic/atomic[atomic], to help the image run as the author intended.  They can also for purely descriptive
purposed and can viewed manually with the _docker inspect <image_name>_ command.

The authoritative source for labels
is the  https://github.com/projectatomic/ContainerApplicationGenericLabels[Container Application Generic Labels] git repository.

==== When are they required?

Labels are never required per-se unless your build system or lifecycle management process requires them.
However, the use of labels is highly recommended for a number of reasons:

* As mentioned above, many container related tools can use the label metadata in meaningful ways often
contributing to a better user experience.
* The label metadata is always visible when inspecting the image.  Therein, users can at least see the
metadata even if their tooling does not make specific use of it.  For example, the RUN label basically
documents how you, as the author of the Dockerfile, expect this image to be run.

==== Descriptive labels

The descriptive labels usually are alpha-numeric strings used to describe some aspect of the image itself.  Examples, might be
the version and release labels which could theoretically just be integer based.
The following table describes labels that are meant to be purely descriptive in nature.

.Descriptive labels
[options="header,footer"]
|===============================================
| Label | Description | Example
| name 	| Name of the Image | _"rhel7/rsyslog"_
| version | Version of the image | _"7.2"_
| release | Release number of the image | _"12"_
| architecture | Architecture for the image | _"x86_64"_
| build-date | Date/Time image was built as https://tools.ietf.org/html/rfc3339[RFC 3339] date-time | _"2015-12-03T10:00:44.038585Z"_
| vendor | Owner of the image | _"Red Hat, Inc."_
| URL | URL with more information about the image | _TBD_
| Summary | Brief description of the image | _TBD_
| Description | Longer description of the image | _TBD_
| vcs-type | The type of version control used by the container source. Generally one of git, hg, svn, bzr, cvs | _"git"_
| vcs-url |URL of the version control repository | _TBD_
| vcs-ref | A 'reference' within the version control repository; e.g. a git commit, or a subversion branch | _"364a...92a"_
| authoritative-source-url |	The authoritative location in which the image is published | _TBD_
| distribution-scope |Intended scope of distribution for image. Possible values are private, authoritive-source-only, restricted, or public  | _private_
| changelog-url | URL of a page containing release notes for the image| _TBD_
|===============================================

==== Action-oriented labels
[[label_action]]
Most action-oriented labels will be a used in the context of a docker command in order for the container to behave in a desired
way.  The following table describes the defined action-oriented labels.

.Action-oriented labels
[options="header,footer"]
|===============================================
| Label | Description | Example
| help | Command to run the help command of the image | _tbd_
| run | Command to run the image | _"docker run -d --privileged --name NAME --net=host --pid=host -v /etc/pki/rsyslog:/etc/pki/rsyslog -v /etc/rsyslog.conf:/etc/rsyslog.conf -v /etc/sysconfig/rsyslog:/etc/sysconfig/rsyslog -v /etc/rsyslog.d:/etc/rsyslog.d -v /var/log:/var/log -v /var/lib/rsyslog:/var/lib/rsyslog -v /run:/run -v /etc/machine-id:/etc/machine-id -v /etc/localtime:/etc/localtime -e IMAGE=IMAGE -e NAME=NAME --restart=always IMAGE /bin/rsyslog.sh"_
| uninstall | Command to uninstall the image | _"docker run --rm --privileged -v /:/host -e HOST=/host -e IMAGE=IMAGE -e NAME=NAME IMAGE /bin/uninstall.sh"_
| install |	Command to install the image | _"docker run --rm --privileged -v /:/host -e HOST=/host -e IMAGE=IMAGE -e NAME=NAME IMAGE /bin/install.sh"_
| stop | Command to execute before stopping container | _tbd_
| debug | Command to run the image with debugging turned on | _tbd_
|===============================================

==== Recommended labels for your project

Labels are critical to properly identifying your image and influencing how it runs.  For the purposes of
identification, we recommend that you at least use the following labels:

* name
* version
* release
* architecture
* vendor

And for actionable labels, we recommend you use at least the following:

* RUN
* INSTALL
* UNINSTALL

These three are the most critical for ensuring that users run the image in the manner you wish.  Furthermore,
tools developed to read and act upon this meta data will work correctly.

In the case that you provide a help file that does not follow the standard of a man page, then the HELP label would also
be prudent.

==== Recommended labels for your OpenShift project

Images that are meant to be run in OpenShift are recommended to contain a set of labels as seen in the https://docs.openshift.org/latest/creating_images/metadata.html[OpenShift Origin documentation]. The labels are namespaced in compliance with the https://docs.docker.com/engine/userguide/labels-custom-metadata/[Docker format]; that is _io.openshift_ for OpenShift and _io.k8s_ for Kubernetes.

See the following example snippet from the https://github.com/openshift/s2i-ruby/blob/master/2.2/Dockerfile#L12[sti-ruby] image:

```
LABEL io.k8s.description="Platform for building and running Ruby 2.2 applications" \
      io.k8s.display-name="Ruby 2.2" \
      io.openshift.expose-services="8080:http" \
      io.openshift.tags="builder,ruby,ruby22"

```

=== Template
// Should we create a default template of sorts


=== Starting your application

Generally the CMD instruction in the Dockerfile is used by docker to start your application
when the image or container is started.  In the planning section, we provided some reasoning
for choosing how to  *_xref:planning_starting_application[start your application]_*.  The following
subsections will show how to implement each choice in your Dockerfile.

==== Calling the binary directly
Being the simplest of the choices, you simply need to call the binary using the CMD instruction or
define an ENTRYPOINT in your Dockerfile.

```
CMD ["/usr/bin/some_binary"]
```

===== Using the CMD Instruction
With CMD, you can identify the default command to run from the image, along with options you want to pass to it.
If there is no ENTRYPOINT in the Dockerfile, the value of CMD is the command run by default when you start the
container image. If there is an ENTRYPOINT in the Dockerfile, the ENTRYPOINT value is run as the command instead,
with the value of CMD used as options to the ENTRYPOINT command.

The CMD instruction can be overridden when you run the image. So, notice the different results from running
mycmd in two different ways:

Any time you add an argument to the end of a docker run command, the CMD instruction inside the container is ignored.
So the second example opens a bash shell instead of running the cat command. If you want to assign a command that
is not overridden by options at the end of a docker run command, use the ENTRYPOINT instruction.

===== Using the ENTRYPOINT Instruction
Like CMD, the ENTRYPOINT instruction lets you define the command executed when you run the container image but it
cannot be overridden by arguments you put at the end of a docker run line. If your Dockerfile includes an
ENTRYPOINT instruction and there is also a CMD instruction, any arguments on the CMD instruction line are passed to
the command defined in the ENTRYPOINT line.

This is the distinct advantage of the ENTRYPOINT instruction over the CMD instruction because the command being run
is not overridden but it can be subsidized.  Suppose you have an ENTRYPOINT instruction that displays two files.
You could easily add an additional file to be displayed by adding it to the docker run command.

You can override the ENTRYPOINT command by defining a new entrypoint with the --entrypoint="" option on the docker
command line.

[[creating_using_a_script]]

==== Using a script
Using a script to start an application is very similar to calling the binary directly. Again, you
use the CMD instruction but instead of pointing at the binary you point at your script that was
injected into the image.  The _registry.access.redhat.com/rhel7/rsyslog_ image uses a script
to start the rsyslogd application. Lets look at the two relevant instructions in its Dockerfile
that make this happen.

The following instruction injects our script (rsyslog.sh) into the image in the _bin_ dir.
```
ADD rsyslog.sh /bin/rsyslog.sh
```

The contents of the script are as follows:

```
#!/bin/sh
# Wrapper to start rsyslog.d with appropriate sysconfig options

echo $$ > /var/run/syslogd.pid

source /etc/sysconfig/rsyslog
exec /usr/sbin/rsyslogd -n $SYSLOGD_OPTIONS
```

Notice how the script does in fact handle environment variables by sourcing the _/etc/sysconfig/rsyslog_
file. And the CMD instruction simply calls the script.

```
CMD [ "/bin/rsyslog.sh" ]
```

[[location_of_supported_files]]
==== Location of Support Files

Docker itself does not limit the developer in deciding where to put the files that are required for starting the container.
Those files might be either starting script, configuration files, template files that are evaluated once the container is started or anything else.
General rule is that if the file can be provided by RPM, it should be provided by RPM. That is also reason why the files that are not provided by RPM
should be put into the same location as if they would come from RPM, because they might eventually end in the RPM.

Whatever the location will be, it is good idea to use similar location to similar container images, for example when a PostgreSQL container image
uses some schema, MariaDB should use a similar schema, because that is what users will expect.

The recommended location and naming scheme is the following (MySQL taken as an example):

.Location of Support Files
[cols="2*", options"header"]
|===
|Location
|Description

|`/usr/bin/run-mysqld`
|Main executables that users usually use; one of them is usually set as default CMD

|`/usr/libexec/container-setup`
|Script that is run during container build to prepare container content; with this command we can run only one command instead of having a complicated scripts directly in the Dockerfile

|`/etc/my.cnf`
|Main config file for the daemon, the location of the config file should be the same as in RPM, because it is what users expect

|`/usr/share/container-scripts/mysql/my-tuning.cnf.template`
|Template for another config file, its content may be evaluated using `envsubst` utility, so concrete values are set according to environment variables given as argument to `docker run` command

|`/var/lib/mysql/data`
|Path to the data, that is often a docker VOLUME; the `data` part is important so the volume-mounted directory does not have a root-owned parent

In order to have the Dockerfile clean, it is good practice to put all the files into one directory and use
their final location under that directory. In case of the MySQL example above, it might look like this:

```
ls root/
/etc/my.cnf
/usr/bin/run-mysqld
/usr/libexec/container-setup
/usr/share/container-scripts/mysql/my-tuning.cnf.template
/var/lib/mysql/data
```

Adding all the files in the Dockerfile can be then as simple as this:

```
COPY root /
```

The source files may be stored on FTP or some other medium that does not keep UNIX file attributes,
so the Dockerfile or `container-setup` script should make sure the files will have proper attributes set,
like that files in `/usr/bin/*` are executable, etc.

[[creating_using_systemd]]
==== Using systemd "inside the container"

Extending our example from link:#creating_using_a_script[starting an application with a script], the rsyslog
image was started with a script.  We could easily use systemd to start the application.  To use systemd
to start a service that has a unit file, we need to tell systemd to enable the service and then let the
init process handle the rest. So instead of the ADD instruction used earlier, we would use a RUN
instruction to enable the service.

```
RUN systemctl enable rsyslog
```

And then we need to change the CMD instruction to call _/usr/sbin/init_ to let systemd take over.

```
RUN /usr/sbin/init
```

==== Using systemd to control containers

The control mechanism for most docker functions is done via the docker commands or something like
the atomic application which simplifies the management of containers and images for users.  But in
a non-development environment, you may wish to treat your containers more like traditional services
or applications.  For example, you may wish to have your containers start in a specific order on
boot-up.  Or perhaps you wish to be able to restart (or recycle) a container because you have changed
its configuration file.

There are several approaches to these sorts of function.  You can make sure a specific container
always starts on boot-up using the --restart switch with the docker command line when you initially
run the image.  There are also orchestration platforms like Kubernetes that will allow you to determine the
start up order of containers even when they are distributed.  But in the case where all the containers
reside on a single node, systemd might just be exactly the solution.  Like with traditional services,
systemd is capable of making sure services both start and in the order they are specified.  Moreover,
any issues with startup or the container are logged like any other system service.

When using systemd to manage your containers, you are really using systemd to call docker commands (and
subsequently the docker daemon) to perform the actions.  Therefore, once you commit to using systemd
to control a container, you will need to make sure that all start, stop, and restart actions are
conducted with systemd.  Failure to do so essentially decouples the docker daemon and systemd causing
systemd to be out of sync.

In review, systemd is a good solution for:

* host system services such as agents and long-running services
* logging via journald
* service dependant management
* traditional service management vis _systemctl_
* multi-container applications with dependencies on the same node


The configuration file below is a sample service file that can be used and edited to control your image or
container.  In the [Unit] section, you can declare other services needed by your image including the cases where
those services are also images.

.Sample template for a systemd service file
```
[Unit]
After=docker.service
Requires=docker.service
PartOf=docker.service
After=[cite another service]
Wants=[cite another service]

[Service]
EnvironmentFile=[path to configuration file]
ExecStartPre=-[command to execute prior to starting]
ExecStart=[command to execute for start]
ExecStartPost=/usr/bin/sleep 10
ExecStop=[command to execute for stop]
Restart=always

[Install]
WantedBy=docker.service
```

In the [Service] section, you can also declare the actual commands that should be run prior to start, in the
case of start, and in the case of stop.  These commands can either be straight base commands or docker run (or stop)
commands as well.  Finally, if you are using a well made image that contains labels like STOP or RUN, you
could also use the atomic command.  For example, a start command could simply be:

```
atomic run <image_name>
```

This works because the actual docker command to run that image is part of the image's metadata and atomic is
capable of extracting it.

The [Service] section also has an option for EnvironmentFile.  In a traditional, non-containerized systemd service,
this configuration file resides in _/etc/sysconfig/<service_name>_.  In the case of a containerized application,
these configuration files are not always configurable and therefore do not reside on the host's filesystem.  And
in the case of where they are configurable, the EnvironmentFile is usually more important to how the service
application is started.  If you are link:#planning_use_systemd[starting the application] within an image
with systemd, then systemd will use _/etc/sysconfig/<service_name>_ within the image itself.

For more information on writing unit files see link:https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/sect-Managing_Services_with_systemd-Unit_Files.html[Managing Systemd unit files].

// Help section
=== Creating a Help file
include::help.adoc[]


=== Creating a Changelog

=== The Dockerfile linter

==== What is the linter?

The Dockerfile-lint is a rule based link:https://en.wikipedia.org/wiki/Lint_(software)['linter'] for verifying Dockerfiles. The rules are used to check file syntax and best practice options for things such as:

* Was yum clean up evoked after a package installation?
* In the RUN section did the writer link commands via semicolons or double ampersands?

These are determined by the rules author and are typically defined by best practices and writer requirements. The input rules are defined via a set of link:http://www.yaml.org/[yaml files].

At the time of this writing, there are a number of templates from base to automated build configurations.

==== Where do I get the linter and how do I install it?

There are two iterations of the linter.

The CLI version of the project is available here:
link:https://github.com/projectatomic/dockerfile_lint[Dockerfile_lint]

The online version of the project is available here:
link:https://access.redhat.com/labs/linterfordockerfile/#/[Online linter]

For the next section, we will assume that the CLI version of the linter will be installed manually via link:https://docs.npmjs.com/[npm] using the following commands:

```
git clone https://github.com/projectatomic/dockerfile_lint/
cd dockerfile_lint
npm install
```
==== Where do I get the templates?

===== Built in Templates
In the config directory, there are two base ruleset files. If the dockerfile_lint is executed without -r these are the base rules used.

* dockerfile_lint/config/default_rules.yaml

* dockerfile_lint/config/base_rules.yaml

===== Addition Types of templates

In the sample_rules directory there are some included templates for OpenShift and an example base template.

* Basic rules - dockerfile_lint/sample_rules/basic_rules.yaml

This set of rules is a basic catchall of your typical Dockerfile. Things such as yum cache clean up and command execution etiquette are checked. These are the rules we will be referencing below.

* OpenShift Template - dockerfile_lint/sample_rules/openshift.yaml

In addition to testing the semantics of the basic template from above, The OpenShift template checks for some link:https://docs.openshift.org/latest/creating_images/metadata.html#defining-image-metadata[required OpenShift labels] specific to its use.

==== How do I read and customize the templates?

The filename of the basic template is included in the command above: sample_rules/basic_rules.yaml

The rules are implemented using regular expressions matched on instruction of the dockerfile. The rule file has 3 sections: *a profile section, a line rule section and a required instructions section.*

===== Profile Section

The profile section gives information about the rule file. This is the name identifier and description for the profile. This information should help users to identify an applicable template.
```
profile:
  name: "Default"
  description: "Default Profile. Checks basic syntax."
  includes:
    - recommended_label_rules.yaml
```
An excerpt from the rules shows how includes are defined:

```
includes:
  - recommended_label_rules.yaml
```
The include section allows for chaining rulesets of multiple sources. In the above example the recommended_label_rules.yaml is processed in addition to its source.

===== Line Rule Section

This section contains rules match on a given instruction in the dockerfile. The line rules do the bulk of the dockerfile parsing.

The example below shows rules to run against the 'FROM' instruction.
```
  FROM registry.access.redhat.com/rhel7:latest
```

The excerpt below checks for the latest flag in the 'FROM' line.

```
line_rules:
  FROM:
    paramSyntaxRegex: /^[a-z0-9./-]+(:[a-z0-9.]+)?$/
      rules:
        -
          label: "is_latest_tag"
          regex: /latest/
          level: "error"
          message: "base image uses 'latest' tag"
          description: "using the 'latest' tag may cause unpredictable builds. It is recommended that a specific tag is used in the FROM line or *-released which is the latest supported release."
          reference_url:
            - "https://docs.docker.com/reference/builder/"
            - "#from"
```
Here is another example that parses the 'RUN' line.
```
  RUN yum -y --disablerepo=\* --enablerepo=rhel-7-server-rpms install yum-utils && \
    yum-config-manager --disable \* && \
    yum-config-manager --enable rhel-7-server-rpms && \
    yum clean all

    RUN yum -y install file open-vm-tools perl open-vm-tools-deploypkg net-tools && \
    yum clean all
```

The regex below checks to see if the yum command has been issued. If it has, check to see if yum clean all has been run as well.
```
  RUN:
    paramSyntaxRegex: /.+/
      rules:
        -
           label: "no_yum_clean_all"  #This is a short description of the rule
           regex: /yum(?!.+clean all|.+\.repo)/g  #regex the linter is attempting to match
           level: "warn" # warn, error or info: These results will define how the linter exits
           message: "yum clean all is not used"
           description: "the yum cache will remain in this layer making the layer unnecessarily large"
           reference_url:
             - "http://docs.projectatomic.io/container-best-practices/#"
             - "_clear_packaging_caches_and_temporary_package_downloads"
            # Lastly, any best practice documentation that may be pertinent to the rule
```
===== Required Instructions Section

While the line rules section uses regex the required instructions looks for the instantiation of the instruction.

```
required_instructions:
  -
    instruction: "EXPOSE"
    count: 1
    level: "info"
    message: "There is no 'EXPOSE' instruction"
    description: "Without exposed ports how will the service of the container be accessed?"
    reference_url:
      - "https://docs.docker.com/reference/builder/"
      - "#expose"
```

==== How do I use the linter?

Execution of the CLI version of the linter may look like this:

```
dockerfile_lint -f /path/to/dockerfile -r sample_rules/basic_rules.yaml
```

Here is some sample output from the command above:

```
--------ERRORS---------

ERROR: Maintainer is not defined. The MAINTAINER line is useful for identifying the author in the form of MAINTAINER Joe Smith <joe.smith@example.com>.
Reference -> https://docs.docker.com/reference/builder/#maintainer

--------INFO---------

INFO: There is no 'ENTRYPOINT' instruction. None.
Reference -> https://docs.docker.com/reference/builder/#entrypoint

```
By default, the linter runs in strict mode (errors and/or warnings result in non-zero return code). Run the command with '-p' or '--permissive to run in permissive mode:
```
dockerfile_lint  -p -f /path/to/dockerfile
```
This allows for quick and automated testing as what is informational and what needs to be addressed immediately.


=== Dockerfiles

==== Location

The Dockerfiles for many of the public images are hosted in git repositories where users can view
them.  This also allows users to customize them as well.

It is also good practice to include the Dockerfile in the image itself.  Some distributions have begun to
include an image's Dockerfile in the directory _/root/buildinfo_.  Consider following the same
approach to make sure your Dockerfiles can be easily found.

Upstream Dockerfiles should be hosted in a public GIT repository, for example https://github.com[GitHub]. Ideally, the repository should be created under the organization relevant to a particular project. For example, http://www.softwarecollections.org[Software Collections] Dockerfiles are available under the GitHub https://github.com/sclorg[sclorg] organization.

==== Images

Upstream Docker images, such as CentOS and Fedora base images and layered images based on these, should be publicly available on https://registry.hub.docker.com/[Docker Hub].

For details on using the Docker Hub registry, see https://docs.docker.com/userguide/dockerimages/[Docker User Guide].

==== Content

Docker is a platform that enables applications to be quickly assembled from components. When creating Docker images, think about the added value you can provide potential users with. The intention should always be bringing some added functionality on top of plain package installation.

As an example, take this https://github.com/docker-library/wordpress/blob/618490d4bdff6c5774b84b717979bfe3d6ba8ad1/apache/Dockerfile[Word Press Dockerfile]. After running the image and linking it with a database image such as mysql, you will get a fully operational Word Press instance. In addition, you can also specify an external database.

This exactly is the purpose of using Docker images; instead of laborious installation and configuration of separate components, you simply pull an image from a registry, acquiring a set of tools ready to be used right out-of-the-box.

==== Enabling Necessary Repositories

TBD

// maybe move somewhere RHEL-specific


==== Users
TBD

==== Working Directory
TBD

==== Exposing Ports

The +EXPOSE+ instruction declares the ports on which a container will listen for incoming connections. You should specify ports your application commonly uses; for example, as seen in this https://github.com/openshift/mysql/blob/master/5.5/Dockerfile[mysql] example:

----
EXPOSE 3306
----

IMPORTANT: The TCP/IP port numbers below 1024 are special in that normal users are not allowed to bind on them.

Therefore, for example for Apache server, ports 8080 or 8433 (HTTP or HTTPS) should be exposed. Otherwise, only the root user will be allowed to run Apache server inside a container.


// For information on exposing ports in Software Collection images, see the xref:software_collections[Software Collections] chapter.

==== Logging
TBD

...



=== References

// References to external sites and project-specific guidelines.

Please see the following resources for more information on the Docker container technology and project-specific guidelines.

http://docs.docker.com/[Docker Documentation] -- Detailed information about the Docker platform.

https://github.com/openshift/openshift-docs/blob/master/creating_images/guidelines.adoc#openshift-specific-guidelines[OpenShift Guidelines] -- Guidelines for creating images specific to the OpenShift project.
